\documentclass[../SMLreport_template.tex]{subfiles}
 
\begin{document}
\subsection{Introduction}

Bootstrap aggregating or Bagging is an ensemble method proposed by Leo Breiman (Bagging Predictors 1996), to help reduce the variance of a predictor. As stated before, reducing the variance of a predictor, and subsequently reducing overfitting of the model, helps improve the accuracy of predictions on unseen data through better generalisation. Bagging is able to reduce the variance of a predictor by using multiple machine learning models, each trained with a subset of data from the training set. The subsets are created by randomly selecting N data instances with replacement from the training dataset, where N is the size of the original training dataset. This method of creating subsets will induce repeated data instances within the individual subsets and consequently, some data instances from the original training set may not appear in a given subset. Having subsets with repeated data instances is an important feature for why Bagging reduces the variance of the ensembled predictor and will be examined in more detail further on. Once the models have been trained, predictions are made by inputting a test sample to each of the models and then a majority voted takes places for classification tasks to decide what the test sample will be predicted as. In the case of regression tasks, a mean average of the modelsâ€™ outputs, usually MSE, are taken as the final prediction. Individually, the models may have high error rates and low accuracies but when combined, the ensemble often sees a higher accuracy when compared to a single model. 

\subsection{Bagging in Depth}
How the training set is manipulated plays a crucial part for Bagging to be an effective ensemble method. Let a training set ${T}$, contain data in the form of $(\bm{x}_{n},y_{n})$, for $n = 1,...,N$, where $\bm{x}$ is a vector of $d$ dimensions of features, $y$ is the corresponding target for the respective vector and $N$ being the size of the training set. We can denote a predictor of $y$ with $\varphi(\bm{x},T)$, where $\varphi$ is the predictor, $\bm{x}$ is the input vector and $T$ is the training set the model used to train on (Brieman 1996). Given a training set $T$ of size $N$, Bagging will create a sequence of $T_{k}$ training sets by randomly selecting $N$ data instances with replacement from $T$. Due to the replacement criteria, this will cause replicated data within a given $T_{k}$, and subsequently, some data instances from $T$ may not appear in $T_{k}$. This method of sampling is called bootstrapping and the $T_{k}$ sets created can be referred to as bootstrapped training sets, $T^{(B)}$. Creating a sequence of training sets in this manner will offer small changes between different instances of $T^{(B)}$. 

Another key constituent for Bagging to be effective is the stability of the learning algorithm used in the ensemble. Small changes in $T$ that cause little changes in $\varphi$ are said to be stable learners, and if small changes in $T$ cause large changes in $\varphi$, it is said that the learning algorithm $\varphi$ is unstable. The criterion of the learners to be unstable is important because if stable learners are used, having multiple models trained with different $T^{(B)}$ will cause the models to have little difference between them, rendering the ensemble useless. Whereas, when using unstable learners, the models using different $T^{(B)}$ will see a larger difference between them. The models within the ensemble that have been trained with a given $T^{(B)}$ may often have lower test accuracies if being compared to the same learner that was trained on $T$, but combining the predictions of these 'different' learners allows the ensemble to reduce variance and overfitting leading to better and more accurate predictions. The characteristics of unstable learners is the reason bootstrapping $T$ causes an improvement in predictions when using this type of ensemble method. Examples of unstable learners are decision trees and neural networks. In the next section we will discuss the Random Forest Classifier, an ensemble method that using Bagging and the decision tree learning algorithm.



\end{document}